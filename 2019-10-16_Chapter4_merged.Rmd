---
title: "Chapter 4"
author: "Sam and Nelson"
date: "16/10/2019"
output: html_document
---

```{r libraries, message=FALSE, warning=FALSE}
#load ALL of the libraries you will need first  
library(tidyverse)  
library(xkcd)  
library(showtext)  
library(sysfonts)  
library(Hiiragi2013)  
library(mouse4302.db)  
library(reshape2)  
library(Hmisc)    
library(ggbeeswarm)  
library(modes)  
library(RColorBrewer)  
library(ggthemes)  
library(magrittr)  
library(plotly)  
library(rgl)  
library(pheatmap)  
library(colorspace)  
library(grid)  
library(ggbio)  
library(GenomicRanges)  
library(flexmix)
library(mclust)
library(mixtools)
library(mosaics)
library(mosaicsExample)
library(here)  
library(HistData)
library(bootstrap)
library(vcd)
```

## Where might you see mixture data?  
* Flow cytometry  
* RNA-seq  
* ChIP-Seq  
* Microbiome  

What do these types of data have in common? Can you think of any other types of data that might be considered a mixture?  

## What is mixture data?    
Heterogeneity (diversity) is a problem with biological data analysis. In other words, biological data often do not follow a unimodal distribution. This requires us to look at the data as a mixture of a few (or many) components. We call this type of data  _*mixture data*_. 

Think of combining 2 distributions
  * It can be a mix of different distributions (Normal+Binomial) or mixture of the same distribution but with different parameters
  * Mixture models can be used to find exptected values, maximum likelihood parameters estimate, and others

In some mixture data sets we are able to detrmine the number of components is small and less than the number of our observations - this is called _*finite mixture data*_.  
In other mixture data sets the number of mixtures is equal to (or greater than) our number of observerations - this type of mixture data is called _*infinite mixture data*_.  


## Chapter Goals 
* Generate mixture data from two normal populations 
* Use expectation-maximization (EM) to determine the number of mixtures in your data
* Learn how analyze mixture data with many zeros (such as Chip-seq data) using zero-inflation
* Learn about the empiral cumulative distribition
* Learn what bootstrapping is and how to use it
* Learn about the gamma-Poisson distribution (looking at you RNA-seq-ers)
* Learn how to choose a data transformation based on a mixture model of your data  

## Finite Mixtures  

Let's model a finite mixture model (based on the normal distribution) with two equal components based on the outcomes of flipping a fair coin. 

The standard normal distrubution has the following inputs:
* n (number of observations)
* mean 
* standard deviation (sd)

![](https://i.ytimg.com/vi/uKRP1ARFxfI/hqdefault.jpg)


In plain english, our code will do the following:  
  * Create a variable coinFlips by "flipping" a coin 10000 times where >0.5 is equal to TRUE (heads) and <=0.5 is equal to FALSE (tails)  
  * We will create a function (oneFlip) to generate 2 normal distributions:
    + When the coin is TRUE (heads), a random normal distrubtion variable with mean 1 and standard   deviation 0.5 will be generated    
    + When the coin is FALSE (tails), a random normal distrubtion variable with mean 3 and standard deviation 0.5 will be generated   
  * We will use vapply to apply oneFlip to coinFlips to generate our 2 component mixture model
    
```{r normalFlips, eval=TRUE}
#set seed so we always get the same outcome
set.seed(1234)
#coinflips flips a fair coin 10000 times
coinflips = (runif(10000) > 0.5)
#table of true/false (heads/tail) outcome from coinflips
table(coinflips)

#one flip generates 2 normal distributions, one from a distribution with mean=1, var=0.25 (sd=0.5), and one from a distribution with mean=3 and var=0.25
oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {
  if (fl) {
   rnorm(1, mean1, sd1)
  } else {
   rnorm(1, mean2, sd2)
  }
}

#vairmix is generated by applying oneflip to coinflips
#if coinflips = true, run if statement (1st distribution), if false run else (2nd distribution)
fairmix = vapply(coinflips, oneFlip, numeric(1))
ggplot(tibble(value = fairmix), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)
```

If you thought vapply was the best thing since for loops, you'll be excited to learn there's an even easier way to accomplish the above using rnorm and the ifelse command.

<span style="color:red"> *WATCH OUT*: </span> The authors suddenly started using variance instead of standard deviation for some reason that is unkown to us.

```{r easierFLips, eval=TRUE}
#set your population means to 1 and 3 
means = c(1, 3)
#set variance to 0.25 (stdev=0.5)
sds = c(0.25, 0.25)

#generate random normal distribution with length (n) equal to the length of coinflips
#if coinflips is TRUE, mean =1 and var =0.25
#if coinflips is FALSE, mean=3 and var=0.25
values = rnorm(length(coinflips),
          mean = ifelse(coinflips, means[1], means[2]),
          sd   = ifelse(coinflips, sds[1],   sds[2]))

ggplot(tibble(value = values), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)
```

Now we can use are more succinct code to ask the question: What would happen if you flipped a coin 1 million times (and also made a histogram with 500 bins to show the outcome)?  

```{r coinFlips1million, eval=TRUE}
#set the seed for sameness
set.seed(1234)
#flip that coin 1000000 times
coinflips_1000000 = (runif(1000000) > 0.5)
#table of true/false (heads/tail) outcome from coinflips
table(coinflips_1000000)

#I always reinitialize vectors and variables in case I previously messed them up 
means = c(1, 3)
sds = c(0.25, 0.25)
#rerun 
values_1000000 = rnorm(length(coinflips_1000000),
          mean = ifelse(coinflips_1000000, means[1], means[2]),
          sd   = ifelse(coinflips_1000000, sds[1],   sds[2]))

#plot it to compare
ggplot(tibble(values_1000000), aes(x = values_1000000)) +
     geom_histogram(fill = "purple", bins = 500)
```

The 2 normal distributions become more distinct with a greater number of trials. Therefore, it can be taken that a larger sample allows better approximation of the number of mixtures.   

But how does the distribution density of our 1000000 coin flips from the population with mean=1 compare to that of the density function for a normal random variable?  

```{r eval=TRUE}
ggplot(dplyr::filter(tibble(values_1000000), coinflips_1000000), aes(x = values_1000000)) +
   geom_histogram(aes(y = ..density..), fill = "purple",
                  binwidth = 0.01) +
   stat_function(fun = dnorm,
          args = list(mean = means[1], sd = sds[1]), color = "red")
```

It looks pretty darn close to the denisty function of the normal distribution.  

Thus, we can write a general formula for density of any mixture model by suming the densities (proportionately).   

> \begin{equation}
> f(x)=\frac{1}{2}\phi_1(x)+\frac{1}{2}\phi_2(x)
> \tag{4.1}
> \end{equation}

Where:  
* ϕ1 is the density of normal N(μ1=1,σ<sup>2</sup>=0.25)
* ϕ2 is the density of normal N(μ1=3,σ<sup>2</sup>=0.25)

```{r eval=TRUE}
fairtheory = tibble(
  x = seq(-1, 5, length.out = 1000),
  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
      0.5 * dnorm(x, mean = means[2], sd = sds[2]))
ggplot(fairtheory, aes(x = x, y = f)) +
  geom_line(color = "red", size = 1.5) + ylab("mixture density")
```

Let's generate a graph for 2 sample mixture model using our knowledge of the general formula for mixture models.  

```{r eval=TRUE}
#our means and variances from above
means = c(1, 3)
sds = c(0.25, 0.25)

#why is the 0.5 in front of dnorm necessary?
#changes height, but is this necessary?
twomixmodel = tibble(
  #generate a sequence from -1 to 5 over 1000 numbers
  x = seq(-1, 5, length.out = 1000),
  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
      0.5 * dnorm(x, mean = means[2], sd = sds[2]))
ggplot(twomixmodel, aes(x = x, y = f)) +
  geom_line(color = "red", size = 1.5) + ylab("mixture density")
```

Can you guess the means of the two sample mixture below. Each component has same variance. Hint: look at R code for chapter.

```{r eval=TRUE}
set.seed(1234)
mystery = tibble(
  #generate 1000 true/false statements
  coinflips = (runif(1e3) > 0.5),
  #generate
  values = rnorm(length(coinflips),
                #mean at 1 if true and 2 if false
               mean = ifelse(coinflips, 1, 2),
               #variance =0.5. std dev = ~0.7
               sd   = ifelse(coinflips, sqrt(.5), sqrt(.5))))
#generate a 30 number sequence based on the min value generated from mystery to the max num generated by mystery
br2 = with(mystery, seq(min(values), max(values), length.out = 30))
#plot
ggplot(mystery, aes(x = values)) +
geom_histogram(fill = "purple", breaks = br2)
```

Now that we know how the data was generated, we can look at the two components of the mixture.

```{r eval=TRUE}
ggplot(mystery, aes(x = values)) +
  #histogram for coinflips=true
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br2) +
  #histogram for coinflips=false
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br2) 
```

This doesn't look quite like our original two mixture model though. This is because in that figure the overlapping bars (purple in this figure) are plotted additively (not on top of each other).

```{r eval=TRUE}

ggplot(mystery, aes(x = values, fill = coinflips)) +
  #plot positive coin flips red
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br2) +
  #plot negative coinflips blue
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br2) +
  #plot the first figure we were guessing (ie. all coinflips)
  geom_histogram(fill = "purple", breaks = br2, alpha = 0.2)
```

Both methods of showing overlapping distributions utilize `coinflips` column of TRUE/FALSE to separate components/groups by colour
  * In real data, these hidden groupings would be missing information.

## The Expectation-Maximization (EM) Algorithm

The EM algorithm is used to infer the value of hidden groupings.  
The EM algorithm alternates between:  
* Pretending to know the probability that each observation belongs to a component (and estimating the distribution paramaters of these components)  
* Pretending to know the distribution parameters of each component (and estimating hte probability that each observation belongs to a given component)  

A simple example:  
A variable Y is measured on a series of objects that come from two groups (but we do not know which group each object belongs to).  
- Start by augmenting the data with a latent (unobserved/hidden)
variable, called U.  
- Use the maximum likelihood approach to find the values of U and the unknown parameters of underlying densities.  
  - We will use the bivariate distribution because we are looking at the distribution of "couples" (Y,U)

* Value of θ maximizing E*(expection) found in _maximization step_
  * Iterations through E & M repeated until E* increases are small -> indicates plateauing likelihood = reaching local max
    * Iteration path _depends on starting point_ but always end up at the same end point (assuming 1 end point)
    * Good practice to repeat procedure several times from different start points to see if same answer received
  * (Better defined by breaking up the EM algorithm in excercise 4.1)
  
How would this work if applied to two unfair coings with p1=0.125, p2=0.25, probability of of picking coin 1 equal to π and probability of picking coin 2 equal to 1-π when we toss the coin twice and record the number of heads K?

```{r eval=TRUE}
#simulate 100 instances of this procedure with π=1/8 and compute the contigency table

probHead = c(0.125, 0.25)
set.seed(1234)
#compute 2 contingency tables, one for pi=1/8 and one for pi=1/4
for (pi in c(1/8, 1/4)) {
  whCoin = sample(2, 100, replace = TRUE, prob = c(pi, 1-pi))
  K = rbinom(length(whCoin), size = 2, prob = probHead[whCoin])
  print(table(K))
}
```
 
These contingecy tables look similar. Thus, it would be hard to identify the distribution parameters based on this data. This illustrates the issue  **indentifiability** -> same observed values can have several explainantions of values (p1, p2, PP). This occurs when there are too many degrees of freedom in the parameters (df = n-1; lose one degree of freedom for every estimated parameter).

Let's look at another example of a mixture of two normals with mean parameters unkown and standard deviation equal to 1 (μ1=?,μ2=?,σ1=σ2=1).

```{r eval=TRUE}
#lets generate that model using R
#the labels are "u"
mus = c(-0.5, 1.5)
#randomly sample 1 or 2, 100 times, with replacement
set.seed(1234)
u = sample(2, 100, replace = TRUE)
#create a random normal distribution of 100 numbers with mean equal to -0.5(1) or 1.5(2)
y = rnorm(length(u), mean = mus[u])
#create tibble duy with mus and means
duy = tibble(u, y)
head(duy)
```

Since we know the labels "u", we can estimate the means by using seperate ML equations for each group.

Notice in the `coinflips` sample, the results were logical (TRUE/FALSE). Here, since the means are unknown the values are 1 or 2.
* If we know the labels (u), can estimate means using separate MLEs for each group.
  * The overall MLE obtained by maximizing the following equation. Or by it's log.
    * Maximization can be split into 2 independent pieces & solved (ex) 2 different MLE equations to solve)
    
\begin{equation}
f(y, u \,|\, \theta) =
\prod_{\{i:\,u_i=1\}} \phi_1(y_i)
\prod_{\{i:\,u_i=2\}} \phi_2(y_i),
\tag{4.3}
\end{equation}

```{r eval=TRUE}
duy %>% group_by(u) %>% summarise(mean(y))
```

Suppose we knew the mixing proportion was λ=1/2, so that we can calculate the so that the density by suming the denisity of the mixture proportionately. What prevents us from solving for the MLE explicitly here?


* To estimate a mixture model, need weighted MLE - weights given by posterior label probabilities
* repetition depends on parameters estimated
* Don't know lables u or mixture proportion λ in real data
  * Need to have an inital guess for labels (u)
  * Need to estiamte parameters
  * Need iterations through EM algorithm - this updates "best guess" for group labels & parameters
    * eventually, this effect plateaus into no improvement per cycling
* Can replace the "hard labels" for u observations (groups 1 & 2)
  * via group membership probabilities summing to 1
  * Probability p(u,x|θ) is the weight/participation of observed x in likelihood function
    * AKA soft averaging - make weighted averages where each pts probabilities used as weight. Don't have       to decide if the point belongs to one group or another
* **Marginal likelihood** for observed y = sum overall possible values of (u) of the densities at (y,u)

  \begin{equation*}
\text{marglike}(\theta;y)=f(y\,|\,\theta)=\sum_u f(y,u\,|\,\theta) \,\text{d}u.
\end{equation*}

* Each iteration (present values marked \*) includes the current best guess for unknown parameters (ex) μ1\*, μ2\*)
  * This is combined into θ* =(μ1\*, μ2\*, λ*)
    * Used to compute expection function E*(θ) (expectation = average/integrate over all possible values of u)
    
\begin{equation*}
E^*(\theta)=E_{\theta^*,Y}[\log p(u,y\,|\,\theta^*)]=\sum_u p(u\,|\,y ,\theta^*)
\log p(u,y\,|\,\theta^*),
\end{equation*}

* Value of θ maximizing E* found in _maximization step_
  * Iterations through E & M repeated until E* increases are small -> indicates plateauing likelihood = reaching local max
    * Iteration path _depends on starting point_ but always end up at the same end point (assuming 1 end point)
    * Good practice to repeat procedure several times from different start points to see if same answer received

Several R packages provide EM implementations, including mclust, EMcluster and EMMIXskew. Choose one and run the EM function several times with different starting values. Then use the function normalmixEM from the mixtools package to compare the outputs.  

```{r eval=F}
EMtest <- Mclust(duy$y, G=2)
EMtest$n
EMtest$df
EMtest$BIC
EMtest$classification
head(duy$u,20)

mixtools <- normalmixEM(duy$y, k=2, lambda=c(0.5,0.5), mu=c(-0.01,0.01), sigma=c(1,1))
paste(mixtools$lambda, "when lambda")
paste(mixtools$mu, "when mean")
paste(mixtools$sigma, "when sd")
paste(mixtools$loglik, "when loglikelihood")
```

These are two different packages, but they have a very similar outcome due to convergence of maximum likelihood.

The EM algorithm is very informative:
  * Shows that hard questions wih many unknowns can be solved via alternating by solving simpler problems
    * Eventually finds estimates of hidden variables
  * Exaple of **soft averaging** (don't need to decide if observation belongs to group 1 or 2 - observation can join multiple groups by using probability membership as weights) -> better estimates of parameters
  * Can use this method for general case of model averaging
    * Consider several models at once to see which is better fit for data -> combine them into a weighted model
    * Weights provided by likelihoods of each model
    
## Zero inflated data

//I dislike that this section doesn't actually go over how to deal with zero inflated data. They just point you to other packages.

Biological data often comes in counts. For example, how many reads from each bacterial species were observed in a given RNA-seq sample?  

Such data can often be seen as a mixture of two scenarios: 
* If the species is not present, the count is zero.
* If the species is present, the number of individuals we observe varies, with a random sampling distribution and this distribution may also include zeros.  

This can be represented by the following equation:

\begin{equation*}
f_{\text{zi}}(y) = \lambda \, \delta(y) + (1-\lambda) \, f_{\text{count}}(y),
\end{equation*}

Where :
  * Deltha = Dirac delta function -> represents prob distribution wit mass at 0
  * 0's in first mixture component (delta) = "structural" for this example
    * This is due to <u>certain species not living in certain habitats</u> (this example)
  * 0's in the second component (fcount)
    * This is due to <u>not finding certain members of a species</u> in some locations (this example)  


The R packages pscl and zicounts provide many examples and functions for working with zero inflated counts.  

Let's look at an example of zero-inflated data from ChIP-seq. These data are sequences of pieces of DNA that are obtained from chromatin immunoprecipitation (ChIP). This technology enables the mapping of the locations along genomic DNA of transcription factors, nucleosomes, histone modifications, chromatin remodeling enzymes, chaperones, polymerases and other protein. Let's use the data measured on chromosome 22 from ChIP-Seq of antibodies for the STAT1 protein and the H3K4me3 histone modification applied to the GM12878 cell line as an example.
    
```{r eval=TRUE}
#construct binTFBS for chr22 data
#binTFBS dataset = binding sites for one chromosome22
constructBins(infile = system.file(file.path("extdata", "wgEncodeSydhTfbsGm12878Stat1StdAlnRep1_chr22_sorted.bam"), package="mosaicsExample"),
    fileFormat = "bam", outfileLoc = "../data/",
    byChr = FALSE, useChrfile = FALSE, chrfile = NULL, excludeChr = NULL,
    PET = FALSE, fragLen = 200, binSize = 200, capping = 0)

constructBins(infile = system.file(file.path("extdata", "wgEncodeSydhTfbsGm12878InputStdAlnRep1_chr22_sorted.bam"), package="mosaicsExample"),
    fileFormat = "bam", outfileLoc = "../data/",
    byChr = FALSE, useChrfile = FALSE, chrfile = NULL, excludeChr = NULL,
    PET = FALSE, fragLen = 200, binSize = 200, capping = 0)
datafiles = c("../data/wgEncodeSydhTfbsGm12878Stat1StdAlnRep1_chr22_sorted.bam_fragL200_bin200.txt",
              "../data/wgEncodeSydhTfbsGm12878InputStdAlnRep1_chr22_sorted.bam_fragL200_bin200.txt")
binTFBS = readBins(type = c("chip","input"), fileName = datafiles)
binTFBS

#create histogram for binTFBS
bincts = print(binTFBS)
ggplot(bincts, aes(x = tagCount)) +
  geom_histogram(binwidth = 1, fill = "forestgreen")
```

We can see that there are many zeros in this data. But is the number of zeros significant given the data (look at the frequency of other small numbers)?

We can see that there appears to be many zeros (based on bar height). However, we can't be sure if the number of zeros is really high because of the small frequencies of many other numbers (1, 2, ...).

Let's look at the log (base10) transformed histogram and estimate the proportion of bins with zero counts.

```{r eval=TRUE}
ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +
   geom_histogram(binwidth = 1, fill = "forestgreen")

##estimate the proportion of bins with zero counts
```
Is this a significant number of zeros? The proportion of zeros in the data doesn't look significant based on the counts of other numbers (is this true - we're not sure).
 
## More than two components

Let's look at an example of a mixture model with more than two components. What sort of histogram do we get when when weighing N=7,000 nucleotides obtained from mixtures of deoxyribonucleotide monophosphates (each type has a different weight, measured with the same standard deviation sd=3).

```{r eval=TRUE}
masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 3
set.seed(1234)
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")
```

This kind of looks like 4 separate components. What does this distribution look like compared to the theoretical density distribution?

```{r eval=TRUE}

#can generate theoretical distribution but cant get it to code on top

#generate theoretical distribution using code from figure 4.3 as example
th.density = tibble(
  th.x = seq(min(quadwts), max(quadwts), length.out = 7000),
  ds =0.12 * dnorm(th.x, mean = masses[1], sd = 3) +
      0.38 * dnorm(th.x, mean = masses[2], sd = 3) +
      0.36 * dnorm(th.x, mean = masses[3], sd = 3) +
      0.14 * dnorm(th.x, mean = masses[4], sd = 3))
ggplot() +
  geom_line(data=th.density, aes(x = th.x, y = ds), 
            color = "red", size = 1.5) + 
  ylab("mixture density")
```

What happens if we repeat this exercise with N=1000 nucleotide measurements?

```{r eval=TRUE}
N  = 1000
sd = 3
set.seed(1234)
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")
```

The individual components have become less distinct. Now it looks like there are only 3 separate components to this data.

What happens if we keep N=7000 measurements but make the standard deviation 10?
```{r eval=T}
N  = 7000
sd = 10
set.seed(1234)
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")
```

Now the individual components are even less distinct. It looks like there are only 2 components within our 4 component data.

## Empirical distributions and the nonparametric bootstrap

This type of data deals with the situation where the number of observatins is equal to the number of mixture components.

Let's use Darwin's _Zea mays_ data where he compared heights of 15 self-hybridized and 15 self-crossed plants as example.

```{r eval=T}
#look at heights of plants
ZeaMays$diff

#plot heights of plants
ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +
  geom_linerange(size = 1, col = "forestgreen") + ylim(0, 0.1)
```

In general, the density of a probability distribution is the derivative (if it exists) of the empirical cumulative distribution function (ECDF).

Statistics of our sample, such as the mean, minimum or median, can also be written as a function of the ECDF.  

The **emperical cumulative distribution function (ECDF)** for sample of size n:  

\begin{equation}
\hat{F}_n(x)=  \frac{1}{n}\sum_{i=1}^n {\mathbb 1}_{x \leq x_i},
\tag{4.4}
\end{equation}

ECDF plots allow us to write density of sample as: 

\begin{equation}
\hat{f}_n(x) =\frac{1}{n}\sum_{i=1}^n \delta_{x_i}(x)
\tag{4.5}
\end{equation}

The true sampling distribution of a statistic ^τ is often hard to know as it requires many different data samples from which to compute the statistic.

*Bootstrapping* is a method to approximate the true sample distribution of ^τ creating new samples drawn from the empirical distribution built from the original sample. 

Different samples from F lead to different data, and so to different values of the estimate ^τ: this is called sampling variability. The distribution of all the ^τ’s is the sampling distribution.

We reuse the data (by considering it a mixture distribution of δs) to create new “datasets” by taking samples and looking at the sampling distribution of the statistics ^τ∗ computed on them. This is called the _*nonparametric bootstrap approach*_.

![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/bootstrap-sample.png)


